//! Markdown reporter for benchmark results
//!
//! Generates GitHub-flavored markdown reports suitable for PR comments,
//! documentation, or archival.

use anyhow::Result;
use std::fmt::Write;

use crate::runner::{BenchmarkResults, MetricSummary, ScenarioResult};

/// Markdown format reporter
pub struct MarkdownReporter;

impl MarkdownReporter {
    /// Format benchmark results as Markdown
    pub fn format(results: &BenchmarkResults) -> Result<String> {
        let mut output = String::new();

        // Title with status badge
        let status_badge = if results.passed {
            "![Passed](https://img.shields.io/badge/status-passed-brightgreen)"
        } else {
            "![Failed](https://img.shields.io/badge/status-failed-red)"
        };

        writeln!(output, "# Benchmark Results: {}", results.suite_name)?;
        writeln!(output)?;
        writeln!(output, "{}", status_badge)?;
        writeln!(output)?;

        // Summary section
        writeln!(output, "## Summary")?;
        writeln!(output)?;
        writeln!(output, "| Property | Value |")?;
        writeln!(output, "|----------|-------|")?;
        writeln!(output, "| **Base URL** | {} |", results.base_url)?;
        writeln!(output, "| **Started** | {} |", results.started_at)?;
        writeln!(output, "| **Duration** | {}ms |", results.total_duration_ms)?;
        writeln!(output, "| **Status** | {} |", if results.passed { "✅ Passed" } else { "❌ Failed" })?;
        writeln!(output)?;

        // Configuration section
        writeln!(output, "## Configuration")?;
        writeln!(output)?;
        writeln!(output, "| Setting | Value |")?;
        writeln!(output, "|---------|-------|")?;
        writeln!(output, "| Iterations | {} |", results.config_summary.iterations)?;
        writeln!(output, "| Warmup | {} |", results.config_summary.warmup)?;
        writeln!(output, "| Parallel Contexts | {} |", results.config_summary.parallel_contexts)?;
        writeln!(output, "| Network Profile | {} |", results.config_summary.network_profile)?;
        writeln!(output, "| CPU Slowdown | {}x |", results.config_summary.cpu_slowdown)?;
        writeln!(output)?;

        // Scenario results
        writeln!(output, "## Scenarios")?;
        writeln!(output)?;

        for scenario in &results.scenario_results {
            Self::format_scenario(&mut output, scenario)?;
        }

        // Failures section
        if !results.failures.is_empty() {
            writeln!(output, "## ❌ Failures")?;
            writeln!(output)?;
            for failure in &results.failures {
                writeln!(output, "- {}", failure)?;
            }
            writeln!(output)?;
        }

        // Footer
        writeln!(output, "---")?;
        writeln!(output)?;
        writeln!(
            output,
            "*Generated by [benchmark-harness](https://github.com/example/benchmark-harness)*"
        )?;

        Ok(output)
    }

    fn format_scenario(output: &mut String, scenario: &ScenarioResult) -> Result<()> {
        let status = if scenario.passed { "✅" } else { "❌" };

        writeln!(output, "### {} {}", scenario.scenario_name, status)?;
        writeln!(output)?;

        // Execution info
        writeln!(
            output,
            "> {} successful iterations, {} failed | Duration: {}ms",
            scenario.successful_iterations, scenario.failed_iterations, scenario.duration_ms
        )?;
        writeln!(output)?;

        // Core Web Vitals table
        writeln!(output, "#### Core Web Vitals")?;
        writeln!(output)?;
        writeln!(
            output,
            "| Metric | Min | P50 | P75 | P95 | P99 | Max | Mean | CV |"
        )?;
        writeln!(
            output,
            "|--------|-----|-----|-----|-----|-----|-----|------|-----|"
        )?;

        Self::format_metric_row(output, "**LCP**", &scenario.lcp_summary, "ms")?;
        Self::format_metric_row(output, "**CLS**", &scenario.cls_summary, "")?;

        if let Some(ref inp) = scenario.inp_summary {
            Self::format_metric_row(output, "**INP**", inp, "ms")?;
        }

        writeln!(output)?;

        // Statistical quality
        writeln!(output, "<details>")?;
        writeln!(output, "<summary>Statistical Details</summary>")?;
        writeln!(output)?;

        writeln!(output, "| Metric | Samples | Outliers Removed | Std Dev |")?;
        writeln!(output, "|--------|---------|------------------|---------|")?;
        writeln!(
            output,
            "| LCP | {} | {} | {:.1}ms |",
            scenario.lcp_summary.count,
            scenario.lcp_summary.outliers_removed,
            scenario.lcp_summary.std_dev
        )?;
        writeln!(
            output,
            "| CLS | {} | {} | {:.4} |",
            scenario.cls_summary.count,
            scenario.cls_summary.outliers_removed,
            scenario.cls_summary.std_dev
        )?;

        if let Some(ref inp) = scenario.inp_summary {
            writeln!(
                output,
                "| INP | {} | {} | {:.1}ms |",
                inp.count, inp.outliers_removed, inp.std_dev
            )?;
        }

        writeln!(output)?;
        writeln!(output, "</details>")?;
        writeln!(output)?;

        // Threshold violations
        if !scenario.failures.is_empty() {
            writeln!(output, "**Threshold Violations:**")?;
            for failure in &scenario.failures {
                writeln!(output, "- ❌ {}", failure)?;
            }
            writeln!(output)?;
        }

        Ok(())
    }

    fn format_metric_row(
        output: &mut String,
        name: &str,
        summary: &MetricSummary,
        unit: &str,
    ) -> Result<()> {
        let format_value = |v: f64| -> String {
            if unit == "ms" {
                format!("{:.0}ms", v)
            } else if v < 1.0 {
                format!("{:.4}", v)
            } else {
                format!("{:.2}", v)
            }
        };

        writeln!(
            output,
            "| {} | {} | {} | {} | {} | {} | {} | {} | {:.1}% |",
            name,
            format_value(summary.min),
            format_value(summary.p50),
            format_value(summary.p75),
            format_value(summary.p95),
            format_value(summary.p99),
            format_value(summary.max),
            format_value(summary.mean),
            summary.cv * 100.0
        )?;

        Ok(())
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::runner::ConfigSummary;

    fn create_test_results() -> BenchmarkResults {
        BenchmarkResults {
            suite_name: "Test Suite".to_string(),
            base_url: "https://example.com".to_string(),
            config_summary: ConfigSummary {
                iterations: 30,
                warmup: 3,
                parallel_contexts: 4,
                network_profile: "None".to_string(),
                cpu_slowdown: 1.0,
            },
            scenario_results: vec![ScenarioResult {
                scenario_name: "Homepage".to_string(),
                lcp_summary: MetricSummary {
                    min: 100.0,
                    p25: 150.0,
                    p50: 200.0,
                    p75: 250.0,
                    p95: 300.0,
                    p99: 350.0,
                    max: 400.0,
                    mean: 210.0,
                    std_dev: 50.0,
                    count: 30,
                    cv: 0.24,
                    outliers_removed: 2,
                },
                cls_summary: MetricSummary {
                    min: 0.0,
                    p25: 0.01,
                    p50: 0.02,
                    p75: 0.03,
                    p95: 0.05,
                    p99: 0.08,
                    max: 0.1,
                    mean: 0.025,
                    std_dev: 0.02,
                    count: 30,
                    cv: 0.8,
                    outliers_removed: 0,
                },
                inp_summary: None,
                lcp_samples: vec![100.0, 150.0, 200.0],
                cls_samples: vec![0.01, 0.02, 0.03],
                successful_iterations: 30,
                failed_iterations: 0,
                duration_ms: 5000,
                passed: true,
                failures: Vec::new(),
            }],
            total_duration_ms: 5000,
            passed: true,
            failures: Vec::new(),
            started_at: "2024-01-01T00:00:00Z".to_string(),
        }
    }

    #[test]
    fn test_markdown_contains_title() {
        let results = create_test_results();
        let output = MarkdownReporter::format(&results).unwrap();

        assert!(output.contains("# Benchmark Results: Test Suite"));
    }

    #[test]
    fn test_markdown_contains_tables() {
        let results = create_test_results();
        let output = MarkdownReporter::format(&results).unwrap();

        // Should have markdown tables
        assert!(output.contains("|"));
        assert!(output.contains("---"));
    }

    #[test]
    fn test_markdown_contains_status_badge() {
        let results = create_test_results();
        let output = MarkdownReporter::format(&results).unwrap();

        assert!(output.contains("![Passed]"));
    }

    #[test]
    fn test_markdown_shows_failed_badge() {
        let mut results = create_test_results();
        results.passed = false;

        let output = MarkdownReporter::format(&results).unwrap();

        assert!(output.contains("![Failed]"));
    }

    #[test]
    fn test_markdown_contains_metrics() {
        let results = create_test_results();
        let output = MarkdownReporter::format(&results).unwrap();

        assert!(output.contains("LCP"));
        assert!(output.contains("CLS"));
        assert!(output.contains("P95"));
    }

    #[test]
    fn test_markdown_has_details_section() {
        let results = create_test_results();
        let output = MarkdownReporter::format(&results).unwrap();

        assert!(output.contains("<details>"));
        assert!(output.contains("Statistical Details"));
        assert!(output.contains("</details>"));
    }
}
